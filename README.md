# asl-video-pipeline
Â A full Python pipeline that transcribes speech from video, converts it into ASL gloss, and renders a signing avatar back onto the original video using poseâ€‘based animations.

 # ASL Video Pipeline

A full Python pipeline that:

1. Extracts & transcribes speech from a video (using WhisperX)  
2. Converts English text into ASL gloss (via the OpenAI Chat API)  
3. Generates pose JSONs and renders a signing avatar back onto the original video  

---

## ğŸ“¦ Prerequisites

- **PythonÂ 3.10.8**  
- `git`, `pip`  
- FFmpeg (for MoviePy audio/video I/O)  
- (Optional) OpenPose if you wish to regenerate raw pose data yourself  

---

## ğŸ”§ Installation

```bash
# 1) Clone this repo and enter it
git clone https://github.com/your-username/asl-video-pipeline.git
cd asl-video-pipeline

# 2) Create & activate a PythonÂ 3.10.8 virtual environment
python3.10 -m venv .venv

# macOS/Linux:
source .venv/bin/activate
# Windows (PowerShell):
.venv\Scripts\Activate.ps1

# 3) Upgrade pip & install dependencies
pip install --upgrade pip
pip install -r requirements.txt

## ğŸ—‘ï¸ Cleanup

```bash
rm -rf output/

ğŸ“„ License
This project is licensed under the MIT License.

âš ï¸ Data Notice
This project uses pose sequences derived from the ASLâ€‘Citizen dataset (Microsoft Research).
Due to that datasetâ€™s licensing policy, all_processed_poses.pkl is not included here.

To reproduce the perâ€‘frame JSONs yourself, you must:

Obtain the ASLâ€‘Citizen data directly from Microsoft Research under their terms.

Place the resulting all_processed_poses.pkl into the data/ folder.

Once in place, the pipeline will automatically generate the JSONs needed for avatar rendering.

```bash
rm -rf output/
